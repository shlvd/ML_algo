{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import graph_objs as go\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "def plotly_df(df, title = ''):\n",
    "    data = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        trace = go.Scatter(\n",
    "            x = df.index,\n",
    "            y = df[column],\n",
    "            mode = 'lines',\n",
    "            name = column\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = dict(title = title)\n",
    "    fig = dict(data = data, layout = layout)\n",
    "    iplot(fig, show_link=False)\n",
    "\n",
    "dataset = pd.read_csv('hour_online.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "plotly_df(dataset, title = \"Online users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(series, n):\n",
    "    return np.average(series[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average(dataset.Users, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMovingAverage(series, n):\n",
    "\n",
    "    \"\"\"\n",
    "    series - dataframe with timeseries\n",
    "    n - rolling window size \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rolling_mean = series.rolling(window=n).mean()\n",
    "\n",
    "    # При желании, можно строить и доверительные интервалы для сглаженных значений\n",
    "    #rolling_std =  series.rolling(window=n).std()\n",
    "    #upper_bond = rolling_mean+1.96*rolling_std\n",
    "    #lower_bond = rolling_mean-1.96*rolling_std\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(n))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    #plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "    #plt.plot(lower_bond, \"r--\")\n",
    "    plt.plot(dataset[n:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(dataset, 24) \n",
    "plotMovingAverage(dataset, 24*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(series, weights):\n",
    "    result = 0.0\n",
    "    weights.reverse()\n",
    "    for n in range(len(weights)):\n",
    "        result += series[-n-1] * weights[n]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_average(dataset.Users, [0.6, 0.2, 0.1, 0.07, 0.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for alpha in [0.3, 0.05]:\n",
    "        plt.plot(exponential_smoothing(dataset.Users, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(dataset.Users.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # прогнозируем\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for alpha in [0.9, 0.02]:\n",
    "        for beta in [0.9, 0.02]:\n",
    "            plt.plot(double_exponential_smoothing(dataset.Users, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(dataset.Users.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "\n",
    "    \"\"\"\n",
    "    Модель Хольта-Винтерса с методом Брутлага для детектирования аномалий\n",
    "    https://fedcsis.org/proceedings/2012/pliks/118.pdf\n",
    "\n",
    "    # series - исходный временной ряд\n",
    "    # slen - длина сезона\n",
    "    # alpha, beta, gamma - коэффициенты модели Хольта-Винтерса\n",
    "    # n_preds - горизонт предсказаний\n",
    "    # scaling_factor - задаёт ширину доверительного интервала по Брутлагу (обычно принимает значения от 2 до 3)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "    def initial_trend(self):\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "\n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # вычисляем сезонные средние\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # вычисляем начальные значения\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []\n",
    "        self.Smooth = []\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "\n",
    "        seasonals = self.initial_seasonal_components()\n",
    "\n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # инициализируем значения компонент\n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "\n",
    "                self.PredictedDeviation.append(0)\n",
    "\n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "\n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "\n",
    "                continue\n",
    "            if i >= len(self.series): # прогнозируем\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "\n",
    "                # во время прогноза с каждым шагом увеличиваем неопределенность\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "\n",
    "            else:\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "\n",
    "                # Отклонение рассчитывается в соответствии с алгоритмом Брутлага\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "\n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i % self.slen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def timeseriesCVscore(x):\n",
    "    # вектор ошибок\n",
    "    errors = []\n",
    "\n",
    "    values = data.values\n",
    "    alpha, beta, gamma = x\n",
    "\n",
    "    # задаём число фолдов для кросс-валидации\n",
    "    tscv = TimeSeriesSplit(n_splits=3) \n",
    "\n",
    "    # идем по фолдам, на каждом обучаем модель, строим прогноз на отложенной выборке и считаем ошибку\n",
    "    for train, test in tscv.split(values):\n",
    "\n",
    "        model = HoltWinters(series=values[train], slen = 24*7, alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n",
    "        model.triple_exponential_smoothing()\n",
    "\n",
    "        predictions = model.result[-len(test):]\n",
    "        actual = values[test]\n",
    "        error = mean_squared_error(predictions, actual)\n",
    "        errors.append(error)\n",
    "\n",
    "    # Возвращаем средний квадрат ошибки по вектору ошибок \n",
    "    return np.mean(np.array(errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.Users[:-500] # отложим часть данных для тестирования\n",
    "\n",
    "# инициализируем значения параметров\n",
    "x = [0, 0, 0] \n",
    "\n",
    "# Минимизируем функцию потерь с ограничениями на параметры\n",
    "opt = minimize(timeseriesCVscore, x0=x, method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1)))\n",
    "\n",
    "# Из оптимизатора берем оптимальное значение параметров\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.Users\n",
    "model = HoltWinters(data[:-128], slen = 24*7, alpha = alpha_final, beta = beta_final, gamma = gamma_final, n_preds = 128, scaling_factor = 2.56)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHoltWinters():\n",
    "    Anomalies = np.array([np.NaN]*len(data))\n",
    "    Anomalies[data.values<model.LowerBond] = data.values[data.values<model.LowerBond]\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.plot(model.result, label = \"Model\")\n",
    "    plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n",
    "    plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n",
    "    plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, y2=model.LowerBond, alpha=0.5, color = \"grey\")\n",
    "    plt.plot(data.values, label = \"Actual\")\n",
    "    plt.plot(Anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    plt.axvspan(len(data)-128, len(data), alpha=0.5, color='lightgrey')\n",
    "    plt.grid(True)\n",
    "    plt.axis('tight')\n",
    "    plt.legend(loc=\"best\", fontsize=13);\n",
    "\n",
    "plotHoltWinters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise = np.random.normal(size=1000)\n",
    "with plt.style.context('bmh'):  \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(white_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotProcess(n_samples=1000, rho=0):\n",
    "    x = w = np.random.normal(size=n_samples)\n",
    "    for t in range(n_samples):\n",
    "        x[t] = rho * x[t-1] + w[t]\n",
    "\n",
    "    with plt.style.context('bmh'):  \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(x)\n",
    "        plt.title(\"Rho {}\\n Dickey-Fuller p-value: {}\".format(rho, round(sm.tsa.stattools.adfuller(x)[1], 3)))\n",
    "\n",
    "for rho in [0, 0.6, 0.9, 1]:\n",
    "    plotProcess(rho=rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "\n",
    "        y.plot(ax=ts_ax)\n",
    "        ts_ax.set_title('Time Series Analysis Plots')\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n",
    "\n",
    "        print(\"Критерий Дики-Фуллера: p=%f\" % sm.tsa.stattools.adfuller(y)[1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "    return \n",
    "\n",
    "tsplot(dataset.Users, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invboxcox(y,lmbda):\n",
    "    # обрабтное преобразование Бокса-Кокса\n",
    "    if lmbda == 0:\n",
    "        return(np.exp(y))\n",
    "    else:\n",
    "        return(np.exp(np.log(lmbda*y+1)/lmbda))\n",
    "\n",
    "data = dataset.copy()\n",
    "data['Users_box'], lmbda = scs.boxcox(data.Users+1) # прибавляем единицу, так как в исходном ряде есть нули\n",
    "tsplot(data.Users_box, lags=30)\n",
    "print(\"Оптимальный параметр преобразования Бокса-Кокса: %f\" % lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Users_box_season'] = data.Users_box - data.Users_box.shift(24*7)\n",
    "tsplot(data.Users_box_season[24*7:], lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Users_box_season_diff'] = data.Users_box_season - data.Users_box_season.shift(1)\n",
    "tsplot(data.Users_box_season_diff[24*7+1:], lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = range(0, 5)\n",
    "d=1\n",
    "qs = range(0, 4)\n",
    "Ps = range(0, 5)\n",
    "D=1\n",
    "Qs = range(0, 1)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "best_aic = float(\"inf\")\n",
    "\n",
    "for param in tqdm(parameters_list):\n",
    "    #try except нужен, потому что на некоторых наборах параметров модель не обучается\n",
    "    try:\n",
    "        model=sm.tsa.statespace.SARIMAX(data.Users_box, order=(param[0], d, param[1]), \n",
    "                                        seasonal_order=(param[2], D, param[3], 24*7)).fit(disp=-1)\n",
    "    #выводим параметры, на которых модель не обучается и переходим к следующему набору\n",
    "    except ValueError:\n",
    "        print('wrong parameters:', param)\n",
    "        continue\n",
    "    aic = model.aic\n",
    "    #сохраняем лучшую модель, aic, параметры\n",
    "    if aic < best_aic:\n",
    "        best_model = model\n",
    "        best_aic = aic\n",
    "        best_param = param\n",
    "    results.append([param, model.aic])\n",
    "\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "result_table = pd.DataFrame(results)\n",
    "result_table.columns = ['parameters', 'aic']\n",
    "print(result_table.sort_values(by = 'aic', ascending=True).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = sm.tsa.statespace.SARIMAX(data.Users_box, order=(4, d, 3), \n",
    "                                        seasonal_order=(4, D, 1, 24)).fit(disp=-1)\n",
    "print(best_model.summary())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(best_model.resid[24:], lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"arima_model\"] = invboxcox(best_model.fittedvalues, lmbda)\n",
    "forecast = invboxcox(best_model.predict(start = data.shape[0], end = data.shape[0]+100), lmbda)\n",
    "forecast = data.arima_model.append(forecast).values[-500:]\n",
    "actual = data.Users.values[-400:]\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(forecast, color='r', label=\"model\")\n",
    "plt.title(\"SARIMA model\\n Mean absolute error {} users\".format(round(mean_absolute_error(data.dropna().Users, data.dropna().arima_model))))\n",
    "plt.plot(actual, label=\"actual\")\n",
    "plt.legend()\n",
    "plt.axvspan(len(actual), len(forecast), alpha=0.5, color='lightgrey')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Возвращает словарь, где ключами являются уникальные категории признака cat_feature, \n",
    "    а значениями - средние по real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dataset)\n",
    "data.columns = [\"y\"]\n",
    "\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data[\"hour\"] = data.index.hour\n",
    "data[\"weekday\"] = data.index.weekday\n",
    "data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mean(data, 'weekday', \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(data, lag_start=5, lag_end=20, test_size=0.15):\n",
    "\n",
    "    data = pd.DataFrame(data.copy())\n",
    "    data.columns = [\"y\"]\n",
    "\n",
    "    # считаем индекс в датафрейме, после которого начинается тестовыый отрезок\n",
    "    test_index = int(len(data)*(1-test_size))\n",
    "\n",
    "    # добавляем лаги исходного ряда в качестве признаков\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "\n",
    "    data.index = data.index.to_datetime()\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "\n",
    "    # считаем средние только по тренировочной части, чтобы избежать лика\n",
    "    data['weekday_average'] = map(code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday)\n",
    "    data[\"hour_average\"] = map(code_mean(data[:test_index], 'hour', \"y\").get, data.hour)\n",
    "\n",
    "    # выкидываем закодированные средними признаки \n",
    "    data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # разбиваем весь датасет на тренировочную и тестовую выборку\n",
    "    X_train = data.loc[:test_index].drop([\"y\"], axis=1)\n",
    "    y_train = data.loc[:test_index][\"y\"]\n",
    "    X_test = data.loc[test_index:].drop([\"y\"], axis=1)\n",
    "    y_test = data.loc[test_index:][\"y\"]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepareData(dataset.Users, test_size=0.3, lag_start=12, lag_end=48)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "prediction = lr.predict(X_test)\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(prediction, \"r\", label=\"prediction\")\n",
    "plt.plot(y_test.values, label=\"actual\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Linear regression\\n Mean absolute error {} users\".format(round(mean_absolute_error(prediction, y_test))))\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performTimeSeriesCV(X_train, y_train, number_folds, model, metrics):\n",
    "    print('Size train set: {}'.format(X_train.shape))\n",
    "\n",
    "    k = int(np.floor(float(X_train.shape[0]) / number_folds))\n",
    "    print('Size of each fold: {}'.format(k))\n",
    "\n",
    "    errors = np.zeros(number_folds-1)\n",
    "\n",
    "    # loop from the first 2 folds to the total number of folds    \n",
    "    for i in range(2, number_folds + 1):\n",
    "        print('')\n",
    "        split = float(i-1)/i\n",
    "        print('Splitting the first ' + str(i) + ' chunks at ' + str(i-1) + '/' + str(i) )\n",
    "\n",
    "        X = X_train[:(k*i)]\n",
    "        y = y_train[:(k*i)]\n",
    "        print('Size of train + test: {}'.format(X.shape)) # the size of the dataframe is going to be k*i\n",
    "\n",
    "        index = int(np.floor(X.shape[0] * split))\n",
    "\n",
    "        # folds used to train the model        \n",
    "        X_trainFolds = X[:index]        \n",
    "        y_trainFolds = y[:index]\n",
    "\n",
    "        # fold used to test the model\n",
    "        X_testFold = X[(index + 1):]\n",
    "        y_testFold = y[(index + 1):]\n",
    "\n",
    "        model.fit(X_trainFolds, y_trainFolds)\n",
    "        errors[i-2] = metrics(model.predict(X_testFold), y_testFold)\n",
    "\n",
    "    # the function returns the mean of the errors on the n-1 folds    \n",
    "    return errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTimeSeriesCV(X_train, y_train, 5, lr, mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def XGB_forecast(data, lag_start=5, lag_end=20, test_size=0.15, scale=1.96):\n",
    "\n",
    "    # исходные данные\n",
    "    X_train, X_test, y_train, y_test = prepareData(dataset.Users, lag_start, lag_end, test_size)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    # задаём параметры\n",
    "    params = {\n",
    "        'objective': 'reg:linear',\n",
    "        'booster':'gblinear'\n",
    "    }\n",
    "    trees = 1000\n",
    "\n",
    "    # прогоняем на кросс-валидации с метрикой rmse\n",
    "    cv = xgb.cv(params, dtrain, metrics = ('rmse'), verbose_eval=False, nfold=10, show_stdv=False, num_boost_round=trees)\n",
    "\n",
    "    # обучаем xgboost с оптимальным числом деревьев, подобранным на кросс-валидации\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=cv['test-rmse-mean'].argmin())\n",
    "\n",
    "    # можно построить кривые валидации\n",
    "    #cv.plot(y=['test-mae-mean', 'train-mae-mean'])\n",
    "\n",
    "    # запоминаем ошибку на кросс-валидации\n",
    "    deviation = cv.loc[cv['test-rmse-mean'].argmin()][\"test-rmse-mean\"]\n",
    "\n",
    "    # посмотрим, как модель вела себя на тренировочном отрезке ряда\n",
    "    prediction_train = bst.predict(dtrain)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(prediction_train)\n",
    "    plt.plot(y_train)\n",
    "    plt.axis('tight')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # и на тестовом\n",
    "    prediction_test = bst.predict(dtest)\n",
    "    lower = prediction_test-scale*deviation\n",
    "    upper = prediction_test+scale*deviation\n",
    "\n",
    "    Anomalies = np.array([np.NaN]*len(y_test))\n",
    "    Anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(prediction_test, label=\"prediction\")\n",
    "    plt.plot(lower, \"r--\", label=\"upper bond / lower bond\")\n",
    "    plt.plot(upper, \"r--\")\n",
    "    plt.plot(list(y_test), label=\"y_test\")\n",
    "    plt.plot(Anomalies, \"ro\", markersize=10)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"XGBoost Mean absolute error {} users\".format(round(mean_absolute_error(prediction_test, y_test))))\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_forecast(dataset, test_size=0.2, lag_start=5, lag_end=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
